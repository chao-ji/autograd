# NumPy-based Framework for Automatic Differentiation


## Introduction

This is a project that I've been contemplating over the years but not able to finish until recently (mid 2023). The motivation was to demystify how backprogagation algorithm works in machine learning libriries like TensorFlow, and the basic idea is to try to build a "mini" framework from the ground up based on the general principles of differentiation and the chain rule of derivatives. I chose to implement only the core components while making the framework expressive enough so that it can be used to build common neural network architectures, which can be traind on toy datasets in a reasonable amount of time.

## Design

### Graph, Operations and Tensors

Generally, running a typical deep learning model comes down to two stages:

1. Symbolically describe the computational process as a directed graph (graph construction time).
2. Carry out the computation by "executing" the graph built in the first stage (runtime).

The Graph is composed of a network of interconnected nodes, called Operations (Ops). Each Op is prescribed to perform a specific type of computation, for example, convoluting an input feature map and a filter into an output feature map. In this case, the Op `Conv2D` takes two input Tensors and generates one output Tensor, where a Tensor is just a symbolic representation of a N-d numpy array. Ops can be connected by feeding the output Tensors of one Op as inputs to another Op.

Note that

1. Tensors cannot exist by themselves --- a Tensor is uniquely bound to and generated by a specific parent Operation.
2. Some Ops does not require an input. For example, a `Const` (constant) Op spits out a Tensor whose value is provided at graph construction time.
3. Some Ops may have "side-effects". For example, an `AssignVariable` Op will modify the value of variables in the `Runtime` environment.


Once we are done with building the graph, we can carry out the computation defined collectively by Ops in the graph. Take the following diagram for an example, where the graph is drawn on the left hand side, and the Ops and Tensors are listed in the table on the right hand side.

To "carry out the computation" simply means to execute a specify Op in the graph. Let's say we'd like to execute the `Add` Op `e`, which adds the input Tensors `c:0` and `d:0`, which are recursively computed by the `Pack` Op `c` and `Exp` Op `d`, and eventually trace back to the two "leaf" Ops `a` and `b`.

### Runtime

As we just showed, the execution of an Op requires the concrete values (numpy array) of upstream Ops. So we need to temporily store the values of Tensors in an environment called `Runtime`, so that downstream Ops can either invoke the execution of upstream Ops or simply look up the values of the output Tensors. Basically, a `Runtime` is a dictionary that maps the ID of tensors to their values.

### Backpropagation

For an Op that has `m` inputs and generates `n` outputs, it is expected to receive `n` gradient Tensors from its downstream Ops. A `_grad_func` is defined for each Op that takes the `m` input and `n` gradient Tensors (or a subset of them), and adds additional Ops (Gradient Graph) that lead to the gradient tensors to be backpropagated to each of the `m` inputs.


### Example




## Usage


## Example


## TODO


## Acknowledgement
